{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6c944d-d5d3-441f-9ea0-15e9695304ce",
   "metadata": {},
   "source": [
    "# scGen: a landmark generative model for unseen perturbations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e86d9-c54d-49ec-8e15-c82525538ee3",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d42ec",
   "metadata": {},
   "source": [
    "Single-cell RNA-sequencing (scRNA-seq) has made it possible to measure how thousands of individual cells change their transcriptional programs in response to genetic, chemical, or environmental **perturbations**. However, performing every possible perturbation in the wet lab is infeasible; the search space explodes combinatorially, and many experiments are costly or technically difficult. **Perturbation‐response models** (such as **scGen**) aim to fill this gap by *predicting* how an unobserved condition would rewrite the transcriptome, given (i) gene-expression data from control cells and (ii) knowledge of a related perturbation.  \n",
    "\n",
    "[**scGen**](https://github.com/theislab/scgen) first compresses every cell’s high-dimensional count vector into a handful of latent “coordinates” using a variational auto-encoder (VAE). Because the VAE is trained on both control and stimulated cells, the latent space ends up organised so that “IFN-β vs control” is almost a straight line. Once the model has that line it can:\n",
    "- Learn a single “perturbation vector” δ (the average shift from control) → IFN-β in the latent space.\n",
    "- Add δ to any other control cell, even one it never saw in training (e.g. a held-out CD4 T cell), and decode the point back to gene space to get a de-novo prediction of the stimulated transcriptome.\n",
    "- Generalise across cell types, studies and even species.\n",
    "\n",
    "The goal of this notebook is therefore two-fold:\n",
    "1. **Methodological** – demonstrate a **reproducible benchmarking pipeline** for scGen using the Perturb-Bench toolbox.  \n",
    "2. **Practical** – show you how to inspect and interpret scGen’s predictions with both **statistical metrics** and **biological read-outs**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd55da43",
   "metadata": {},
   "source": [
    "### 1.1.  Why use the [*Kang 2018*](https://figshare.com/articles/dataset/Kang_HM_Subramaniam_M_Targ_S_Nguyen_M_et_al_2017/19397624?file=34464122) dataset?  \n",
    "\n",
    "* **Clean control vs IFN-β stimulation** on human PBMCs (peripheral blood mononuclear cells), generated with a 6-hour cytokine pulse that elicits a strong innate-immune transcriptional response.\n",
    "* **Moderate size (~25 k cells × 15 k genes)** - big enough to show realistic heterogeneity yet small enough to train scGen in minutes on a laptop CPU.\n",
    "* **Rich cell-type annotation** (CD4 T, CD8 T, NK, monocytes, B cells, dendritic cells, …) enables plug-and-play experiments with any lineage.\n",
    "* **Widely used benchmark** in perturbation-response papers (including the original scGen study), so results are easy to compare with the literature.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018b82e",
   "metadata": {},
   "source": [
    "### 1.2.  Overview of the notebook\n",
    "\n",
    "In this hands‑on notebook you will:\n",
    "\n",
    "1. **Load & explore** the *Kang 2018* single‑cell PBMC dataset (control vs IFN‑β).  \n",
    "2. **Prepare** the data → QC, log‑normalise, select 2,000 highly‑variable genes.  \n",
    "3. **Train scGen** *only on CD4 T cells*, holding out the stimulated cells.  \n",
    "4. **Evaluate** predictions with a variety of meaningful metrics:  \n",
    "    - $R^2$ on log‑counts  \n",
    "    - bulk distance measures  \n",
    "    - overlap of the top‑100 differentially‑expressed genes (DEGs).  \n",
    "5. **Plug‑&‑play**: rerun the same pipeline for *any* other cell type via a drop‑down.  \n",
    "6. **Cross-study extrapolation**: use the trained model to predict IFN-β responses in a different dataset.\n",
    "\n",
    "Let’s jump in!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86402f65",
   "metadata": {},
   "source": [
    "## 2. Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d037d-b290-49f3-a2a9-4ca88f17669f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis '}' does not match opening parenthesis '(' (552255415.py, line 45)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(f\"Running on'CPU'\"}\u001b[39m\n                            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m closing parenthesis '}' does not match opening parenthesis '('\n"
     ]
    }
   ],
   "source": [
    "# --- Standard-library utilities ------------------------------------------------\n",
    "import os                 # file-system operations (e.g. os.environ or os.path)\n",
    "import sys                # Python runtime details; we use it for version checks\n",
    "import time               # simple timing / benchmarking of cells\n",
    "from pathlib import Path  # convenient, cross-platform path handling\n",
    "from collections import OrderedDict  # deterministic dicts for reproducible logs\n",
    "import random             # Python RNG (seeded below for reproducibility)\n",
    "import warnings           # warning control (e.g. to suppress scVI warnings)\n",
    "warnings.filterwarnings('ignore')  # suppress warnings (e.g. from scVI or scanpy)\n",
    "\n",
    "# --- Numerical & data-handling stack -------------------------------------------\n",
    "import numpy as np        # dense numerical arrays\n",
    "import pandas as pd       # tabular data frames\n",
    "from pandas.api.types import CategoricalDtype  # categorical data types\n",
    "from scipy import sparse  # sparse matrix utilities\n",
    "from scipy import stats   # statistical functions (e.g. t-tests, correlations)\n",
    "from scipy.spatial.distance import cosine as cosine_dist, cdist  # distance metrics\n",
    "from scipy.sparse import issparse  # check if a matrix is sparse\n",
    "\n",
    "# --- Single-cell analysis ecosystem --------------------------------------------\n",
    "import scanpy as sc       # core single-cell workflow (AnnData, QC, plotting)\n",
    "from scanpy.pl import DotPlot # dot plots for visualising gene expression\n",
    "import anndata            # core data structure for single-cell data (AnnData)\n",
    "import pertpy as pt       # Perturb-Bench helpers: dataset loaders & metrics\n",
    "import scvi               # single-cell variational inference (scVI) for deep learning\n",
    "from scvi import REGISTRY_KEYS  # keys used by scvi-tools / scGen for AnnData metadata\n",
    "\n",
    "# --- Deep-learning backend -----------------------------------------------------\n",
    "import torch                    # PyTorch (Tensor operations)\n",
    "import pytorch_lightning as pl  # PyTorch Lightning (high-level training framework)\n",
    "\n",
    "# --- Machine-learning utilities ------------------------------------------------\n",
    "from sklearn.decomposition import PCA                # dimensionality reduction\n",
    "from sklearn.neighbors import KernelDensity          # density estimation\n",
    "from sklearn.metrics import r2_score                 # R-squared metric\n",
    "from sklearn.metrics import pairwise_distances       # pairwise distance metrics\n",
    "\n",
    "# --- Plotting & visualisation --------------------------------------------------\n",
    "import matplotlib.pyplot as plt  # base plotting library\n",
    "import matplotlib as mpl         # base matplotlib library\n",
    "import seaborn as sns            # higher-level statistical plots\n",
    "\n",
    "# --- Device & runtime configuration --------------------------------------------\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Running on'CPU'\")\n",
    "\n",
    "# --- Miscellaneous settings ----------------------------------------------------\n",
    "import ipywidgets as widgets # interactive widgets for Jupyter notebooks\n",
    "\n",
    "# --- Global reproducibility ----------------------------------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True   # exact determinism (may reduce speed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "scvi.settings.seed = SEED \n",
    "\n",
    "print(f\"Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86ad945",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7cbf28-b521-46a3-9c4b-05685600bf19",
   "metadata": {},
   "source": [
    "## 3. Data Exploration & Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aeb917-f117-446e-bf38-0865ebe4f1f0",
   "metadata": {},
   "source": [
    "### 3.1. Loading and Exploring the *Kang 2018* dataset\n",
    "\n",
    "We begin by loading the *Kang 2018* dataset that will be used throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3578deb5-aea0-4aa7-9a7b-fe4519b1df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata = pt.dt.kang_2018() # downloading the Kang 2018 dataset\n",
    "adata = sc.read('../data/kang_2018.h5ad') # loading the data directly from the file\n",
    "\n",
    "adata # viewing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5fe06e",
   "metadata": {},
   "source": [
    "[`AnnData`](https://anndata.readthedocs.io/) is the de-facto container for single-cell data in Python (used by **Scanpy**, **scvi-tools**, **Perturb-Bench**, and many other packages). Think of it as a tidy “Excel workbook” with one big table of gene expression and several *linked* annotation sheets:\n",
    "\n",
    "| Attribute | Purpose | In **Kang 2018 PBMC** |\n",
    "|-----------|---------|-----------------------|\n",
    "| **`.X`** *(n_obs × n_vars)* | The main numerical matrix – usually raw counts, log-counts, or latent embeddings. Rows = **cells** (*observations*), columns = **genes** (*variables*). | **24,673 cells** × **15,706 genes** (raw counts; we’ll log-normalise below). |\n",
    "| **`.obs`** | Per-cell metadata (`pandas.DataFrame`). | 12 columns – e.g. `cell_type`, `label` (*ctrl* / *stim*), `replicate`, t-SNE coordinates. |\n",
    "| **`.var`** | Per-gene metadata. | A single column `name` (Ensembl IDs). |\n",
    "| **`.layers`** | Alternative expressions of `.X` (e.g. raw counts, denoised counts, predictions). | Empty for now (can contain layers like `log_norm` or `pred`) |\n",
    "| **`.uns`** | Unstructured auxiliary info (colour maps, settings, model fits). | Currently empty; Scanpy will store `log1p` and plotting settings here. |\n",
    "| **`.obsm` / `.varm`** | Multi-dimensional annotations (matrices): embeddings, feature scores. | `X_pca` (50-D PCA) and `X_umap` (2-D UMAP). |\n",
    "| **`.obsp` / `.varp`** | Pairwise matrices (graphs, distances). | Not present yet – we could add neighbourhood graphs after QC. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ebee09",
   "metadata": {},
   "source": [
    "We rename `label` to `condition` and the conditions themselves for improved readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e104a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.rename(columns={\"label\": \"condition\"}, inplace=True) # renaming the 'label' column to 'condition'\n",
    "\n",
    "# Renaming the categories in the 'condition' column\n",
    "if isinstance(adata.obs[\"condition\"].dtype, CategoricalDtype):\n",
    "    adata.obs[\"condition\"] = (\n",
    "        adata.obs[\"condition\"]\n",
    "        .cat.rename_categories({\"ctrl\": \"control\", \"stim\": \"stimulated\"})\n",
    "    )\n",
    "else:\n",
    "    adata.obs[\"condition\"] = (\n",
    "        adata.obs[\"condition\"]\n",
    "        .map({\"ctrl\": \"control\", \"stim\": \"stimulated\"})\n",
    "        .astype(\"category\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2866a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata # viewing the data again after renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d610d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.head(10)  # viewing the first 10 rows of the observation metadata (cell-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var.head(10)  # viewing the first 10 rows of the variable metadata (gene-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf9b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(adata.obs['condition'].value_counts())  # counting the number of cells in each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(adata.obs['cell_type'].value_counts())  # counting the number of cells in each cell type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20059c20",
   "metadata": {},
   "source": [
    "We can also visualise the number of cells per `cell_type` and per `condition` (`control`/`stimulated`), as well as their distribution in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f095c3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot: cells per type × condition\n",
    "count_df = (adata.obs.groupby(['cell_type','condition'], observed=True)['condition']\n",
    "            .count().unstack().fillna(0))\n",
    "count_df.plot.bar(stacked=True, figsize=(8,4))\n",
    "plt.ylabel('cells'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f320fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP provided by dataset\n",
    "sc.pl.umap(adata, color=['condition','cell_type'], wspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b2570a",
   "metadata": {},
   "source": [
    "> **What am I looking at?**  \n",
    "> *Uniform Manifold Approximation and Projection* (UMAP) squeezes our original\n",
    "> 15,706-dimensional gene-expression vectors down to **two coordinates** that we\n",
    "> can plot.  \n",
    "> \n",
    "> * The algorithm keeps **nearby cells near each other** and **far-away cells\n",
    ">   far apart** as best it can, so dense blobs usually correspond to biologically\n",
    ">   coherent populations (here: T, B, NK, monocytes …).  \n",
    "> * The axes themselves have **no physical meaning** - UMAP is about neighbourhood\n",
    ">   structure, not absolute positions. Rotating the plot would tell the same\n",
    ">   story.  \n",
    "> * When we colour by **condition** you can already see that IFN-β (stimulated)\n",
    ">   cells shift away from control cells within each lineage. That global offset\n",
    ">   is exactly the pattern scGen will try to learn and reproduce.  \n",
    "> \n",
    "> UMAP is only a visual aid; scGen learns its **own latent space** with a\n",
    "> variational auto-encoder. The nice thing is that both spaces often agree:\n",
    "> cells that cluster together on UMAP tend to stay close in scGen’s latent\n",
    "> coordinates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d34b46",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3847d8",
   "metadata": {},
   "source": [
    "### 3.2. Preparing the *Kang 2018* dataset for scGen\n",
    "\n",
    "scGen, like most machine-learning models, expects an informative yet compact input matrix. The raw *Kang 2018* object still contains ~15,700 genes, including many that are lowly expressed or uninformative for the IFN-β response. Our four-step preprocessing pipeline therefore:\n",
    "\n",
    "1. **Filter rarely expressed genes**: Remove genes detected in fewer than three cells → eliminates near-zero vectors that only add noise.\n",
    "2. **Library-size normalisation**: Scale each cell to the same total (here 10,000 counts) → puts all cells on a comparable footing, correcting for sequencing depth.\n",
    "3. **Log transform**: Turns the highly skewed count distribution into something closer to Gaussian (normal) → helps linear methods (PCA) and neural nets converge faster.\n",
    "4. **Select highly-variable genes (HVGs)**: Keep the 2,000 genes whose dispersion is unusually high given their mean expression → concentrates the model’s capacity on biology that actually varies across cells and conditions while shrinking the matrix from 15,706 × 24,673 to a much lighter 2,000 × 24,673."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15fe7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.filter_genes(adata, min_cells=3) # filtering genes expressed in at least 3 cells\n",
    "sc.pp.normalize_total(adata, target_sum=1e4) # normalizing total counts per cell to 10,000\n",
    "sc.pp.log1p(adata) # log-transforming the data (log1p = log(x + 1))\n",
    "sc.pp.highly_variable_genes(adata, flavor='seurat', n_top_genes=2000, subset=True) # selecting the top 2000 highly variable genes\n",
    "\n",
    "adata # viewing the data again after pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d986f60",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506dbdd-2392-488b-8d54-b1d93e5cf71c",
   "metadata": {},
   "source": [
    "## 4. Training scGen and Predicting the IFN-β Response of CD4 T cells\n",
    "\n",
    "- **Hold-out strategy**: we temporarily *hide* every CD4 T cell that was measured under IFN-β stimulation. By training on all remaining cells (other lineages + CD4 T controls) we force scGen to *predict* the missing condition instead of memorising it.  \n",
    "- **Model setup**: we tell scGen which column encodes the experimental condition (`control` vs `stimulated`) and which column contains the biological label (`cell_type`). This lets the model learn a *global* perturbation vector while still respecting lineage-specific expression programs.  \n",
    "- **10-epoch training**: enough for convergence on this medium-sized dataset yet fast in a workshop setting, with early stopping so we don’t waste epochs once the loss plateaus.  \n",
    "- **Latent arithmetic**: after training, scGen adds the learned “IFN-β shift” to the latent coordinates of each *control* CD4 T cell, decodes those points back to gene space, and labels them **`predicted`**. The result is a synthetic population that should resemble *real* stimulated CD4 T cells.\n",
    "- **Preview of next section**: we will immediately evaluate these predictions with statistical (R², distances) and biological (DEG overlap) metrics so you can judge how well scGen captured the IFN-β response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e735798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a training set by excluding 'stimulated' cells of the specified cell type (CD4 T cells in this case)\n",
    "celltype_to_train = 'CD4 T cells'\n",
    "\n",
    "# Filtering the AnnData object to create a training set\n",
    "train_adata = adata[~(\n",
    "    (adata.obs['cell_type'] == celltype_to_train) &\n",
    "    (adata.obs['condition'] == 'stimulated')\n",
    ")].copy()\n",
    "\n",
    "pt.tl.Scgen.setup_anndata(train_adata, batch_key='condition', labels_key='cell_type') # Setting up the AnnData object for scGen with batch and labels keys\n",
    "# scGen uses 'condition' as the batch key and 'cell_type' as the labels key.\n",
    "# This means that scGen will learn to generate data conditioned on the 'condition' (batch) and will use 'cell_type' as the labels for the generated data.\n",
    "model = pt.tl.Scgen(train_adata) # Initializing the scGen model with the training data\n",
    "# Training the scGen model with early stopping (to prevent overfitting).\n",
    "# The model will train for a maximum of 10 epochs with a batch size of 32.\n",
    "model.train(max_epochs=10, batch_size=32,\n",
    "            early_stopping=True,\n",
    "            accelerator=('cpu'),\n",
    "            devices=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463089d5",
   "metadata": {},
   "source": [
    "> **Hyper-parameters** are the dials we set **before** training starts (unlike the millions of weights the network learns **during** training). Choosing sensible values keeps training fast **and** prevents the model from over- or under-fitting. Below are the few we explicitly set in this first pass and what they do:\n",
    "> - `max_epochs`: One full pass over the training set counts as an “epoch”. Ten is enough for this medium-sized dataset to converge while still finishing in a couple of minutes on CPU.\n",
    "> - `batch_size`: How many cells the optimiser processes at once. 32 fits comfortably in CPU and typical laptop RAM while giving stable gradient estimates.\n",
    "> - `early_stopping`: Watches validation loss and stops before max_epochs if the model stops improving → saves time & avoids over-fitting.\n",
    "> - `accelerator / devices`: Tells PyTorch-Lightning where to run the computations.\n",
    "> - `batch_key` & `labels_key` (set via `setup_anndata`): Let scGen know what is the perturbation dimension and which biological labels to respect when sampling.\n",
    "> \n",
    "> We’re sticking to sensible library defaults for now, but scGen exposes many more hyper-parameters you might want to explore in the benchmarking part of the tutorial. Here are some of the most important ones:\n",
    "> - **Latent-space size** (`n_latent`, default = 50): smaller → faster, larger → can capture subtle signals.\n",
    "> - **Network width & depth** (`n_hidden`, `n_layers`): trade-off between model capacity and over-fitting.\n",
    "> - **KL-divergence weight** (`kl_weight`): balances reconstruction accuracy vs. latent regularisation.\n",
    "> - **Learning-rate & optimiser**: can speed up convergence dramatically.\n",
    "> - **Dropout / layer-norm options**: regularise the model when data are noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c6b8b",
   "metadata": {},
   "source": [
    "Now that we have trained the scGen model, we can use it to predict the IFN-β response in CD4 T cells. The model will generate synthetic data that simulates how these cells would look under IFN-β stimulation, based on the learned perturbation vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa88389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the 'stimulated' condition for the specified cell type (CD4 T cells in this case)\n",
    "pred, _ = model.predict(ctrl_key='control', stim_key='stimulated',\n",
    "                        celltype_to_predict=celltype_to_train)\n",
    "pred.obs['condition'] = 'predicted' # Setting the condition of the predicted data to 'predicted'\n",
    "\n",
    "ctrl = adata[(adata.obs['cell_type']==celltype_to_train)&(adata.obs['condition']=='control')] # Filtering the AnnData object to create a control set for the specified cell type (CD4 T cells in this case)\n",
    "stim = adata[(adata.obs['cell_type']==celltype_to_train)&(adata.obs['condition']=='stimulated')] # Filtering the AnnData object to create a stimulated set for the specified cell type (CD4 T cells in this case)\n",
    "\n",
    "eval_ad = anndata.concat([ctrl, stim, pred]) # Concatenating the control, stimulated, and predicted data into a single AnnData object for evaluation\n",
    "eval_ad.obs_names_make_unique() # Ensuring that the observation names are unique in the concatenated AnnData object\n",
    "sc.tl.pca(eval_ad, n_comps=30, svd_solver='arpack') # Performing PCA on the concatenated AnnData object to reduce dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e5fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ad # Viewing the concatenated AnnData object after PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(eval_ad.obs['condition'].value_counts())  # counting the number of cells in each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ea7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(eval_ad.obs['cell_type'].value_counts())  # counting the number of cells in each cell type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b59e40",
   "metadata": {},
   "source": [
    "In order to visualise the predictions and see their overlap with the actual stimulated cells of the concatenated AnnData object, we will use the PCA coordinates of the cells. This will allow us to see how well the predicted cells cluster with the actual stimulated cells in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec28c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.pca(eval_ad, color=['condition']) # Plotting PCA results colored by condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32a99b",
   "metadata": {},
   "source": [
    "We will save this concatenated AnnData object with the predictions to a file in case we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedae3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ad.write(\"../data/kang_2018_CD4_scGen_predictions_default.h5ad\")  # Saving the concatenated AnnData object with predictions to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c3dac7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0fac82",
   "metadata": {},
   "source": [
    "## 5. Evaluating scGen Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31e61e",
   "metadata": {},
   "source": [
    "### 5.1. Performance Metrics\n",
    "\n",
    "Our training trick deliberately **withheld the real IFN-β CD4 T cells** from the model. Now that scGen has generated a *synthetic* IFN-β population, we want to know:\n",
    "\n",
    "* **How close are the predictions to the real stimulated cells?** (→ accuracy)\n",
    "* **Are they clearly different from the control cells?** (→ biological signal preserved)\n",
    "\n",
    "To answer this we compare the **three groups of cells** inside `eval_ad`:\n",
    "- `control`: real CD4 T cells sequenced under basal conditions\n",
    "- `stimulated`: real CD4 T cells sequenced after 6 h IFN-β\n",
    "- `predicted`: scGen’s in-silico IFN-β CD4 T cells\n",
    "\n",
    "For every metric below we ask: *“Do `predicted` look more like `stimulated` than like `control`?”*\n",
    "\n",
    "| Metric | What it measures | Intuition / how to read it | Ideal value |\n",
    "|--------|------------------|----------------------------|-------------|\n",
    "| **R²** *(bootstrap)* | Fraction of gene-mean variance explained by the prediction. | 1 = perfect match; 0 = no better than guessing the mean. | → **closer to 1** |\n",
    "| **MSE** *(bootstrap)* | Average squared error of gene means. | Penalises large mistakes; expressed in log-count² units. | → **smaller is better** |\n",
    "| **Pearson r** *(bootstrap)* | Linear correlation between predicted and real gene means. | Captures overall trend regardless of scale. | → **closer to 1** |\n",
    "| **Energy distance (E-distance)** | $$E_d(X,Y)=2\\,\\mathbb E\\|X-Y\\|-\\mathbb E\\|X-X'\\|-\\mathbb E\\|Y-Y'\\|$$ between `predicted` and `stimulated` cells. | Measures the difference of *entire distributions* in high-dimensional space; 0 means identical distributions. | → **closer to 0** |\n",
    "| **Mean–variance KDE (Kernel Density Estimation) distance** | L1 (Manhattan) distance between the 2-D KDEs of *(gene mean, variance)* for `predicted` vs `stimulated`. | Looks at distribution shape, not just centroids. | → **smaller is better** |\n",
    "| **Top-100 DEG Jaccard** | Overlap of the 100 most up/down genes in `stimulated` vs `predicted`. | 0 = no common DEGs, 1 = identical gene lists. | → **closer to 1** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba740c6",
   "metadata": {},
   "source": [
    "> **Bootstrap:** We repeatedly resample cells (80 % each time, 100 rounds) to estimate the variability of R²/MSE/Pearson, giving more robust averages on small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26437103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def mean_var_kde_distance(X1, X2, bandwidth=1.0, grid_size=50):\n",
    "    \"\"\"\n",
    "    Compute the L1 (Manhattan) distance between mean-variance KDEs.\n",
    "    This function computes the mean and variance of each feature in the datasets, constructs a grid of points in the mean-variance space, and evaluates the\n",
    "    kernel density estimates (KDE) for both datasets at these points. The distance is computed as the sum of the absolute differences between the KDEs of the two\n",
    "    datasets over the grid.\n",
    "    If either dataset has no valid points (e.g., all NaNs), the function returns NaN.\n",
    "    The function is useful for comparing the distributions of features in two datasets, particularly in the context of single-cell RNA-seq data where\n",
    "    mean and variance are often used to characterize gene expression profiles.\n",
    "    \"\"\"        \n",
    "    m1, v1 = X1.mean(0), X1.var(0)\n",
    "    m2, v2 = X2.mean(0), X2.var(0)\n",
    "    pts1, pts2 = np.column_stack([m1,v1]), np.column_stack([m2,v2])\n",
    "    ok = ~(np.isnan(pts1).any(1)|np.isnan(pts2).any(1))\n",
    "    pts1, pts2 = pts1[ok], pts2[ok]\n",
    "    if pts1.size==0 or pts2.size==0:\n",
    "        return np.nan\n",
    "    mins = np.minimum(pts1.min(0), pts2.min(0))\n",
    "    maxs = np.maximum(pts1.max(0), pts2.max(0))\n",
    "    xs = np.linspace(mins[0], maxs[0], grid_size)\n",
    "    ys = np.linspace(mins[1], maxs[1], grid_size)\n",
    "    grid = np.column_stack(np.meshgrid(xs, ys)).reshape(-1,2)\n",
    "    kde1 = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(pts1)\n",
    "    kde2 = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(pts2)\n",
    "    d1 = np.exp(kde1.score_samples(grid))\n",
    "    d2 = np.exp(kde2.score_samples(grid))\n",
    "    dx, dy = xs[1]-xs[0], ys[1]-ys[0]\n",
    "    return float(np.abs(d1-d2).sum()*dx*dy)\n",
    "\n",
    "def bootstrap_metrics(X, real_mask, pred_mask, pca, n_boot=100, frac=0.8, seed=SEED):\n",
    "    \"\"\"\n",
    "    Bootstrap metrics for evaluating the performance of a model.\n",
    "    This function computes the R2, MSE, and Pearson correlation coefficients between the predicted and real data.\n",
    "    It uses bootstrapping to estimate the variability of these metrics.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    r_idx, p_idx = np.where(real_mask)[0], np.where(pred_mask)[0]\n",
    "    if len(r_idx)==0 or len(p_idx)==0:\n",
    "        return {k:np.nan for k in ['R2','MSE','Pearson']}\n",
    "    s_r, s_p = int(frac*len(r_idx)), int(frac*len(p_idx))\n",
    "    vals = {'R2':[], 'MSE':[], 'Pearson':[]}\n",
    "    for _ in range(n_boot):\n",
    "        r = rng.choice(r_idx, s_r, replace=True)\n",
    "        p = rng.choice(p_idx, s_p, replace=True)\n",
    "        Y, Xp = X[r].mean(0), X[p].mean(0)\n",
    "        d = Xp - Y\n",
    "        vals['MSE'].append(np.mean(d**2))\n",
    "        ss_res = ((Y - Xp)**2).sum(); ss_tot = ((Y - Y.mean())**2).sum()\n",
    "        vals['R2'].append(1-ss_res/ss_tot if ss_tot>0 else np.nan)\n",
    "        vals['Pearson'].append(stats.pearsonr(Xp, Y)[0])\n",
    "    return {k:np.nanmean(v) for k,v in vals.items()}\n",
    "\n",
    "def energy_d(X1, X2, max_cells=2_000, seed=SEED):\n",
    "    \"\"\"\n",
    "    Multivariate Energy distance   E_d(X,Y)=2·E‖X−Y‖−E‖X−X'‖−E‖Y−Y'‖\n",
    "    -----------------------------------------------------------------\n",
    "    * works for dense **or** sparse matrices\n",
    "    * optional uniform subsampling keeps RAM small\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # dense float64\n",
    "    X1 = X1.toarray() if issparse(X1) else np.asarray(X1, dtype=np.float64)\n",
    "    X2 = X2.toarray() if issparse(X2) else np.asarray(X2, dtype=np.float64)\n",
    "\n",
    "    # subsample rows if needed\n",
    "    if X1.shape[0] > max_cells:\n",
    "        X1 = X1[rng.choice(X1.shape[0], max_cells, replace=False)]\n",
    "    if X2.shape[0] > max_cells:\n",
    "        X2 = X2[rng.choice(X2.shape[0], max_cells, replace=False)]\n",
    "\n",
    "    # pair-wise Euclidean distances\n",
    "    xy = pairwise_distances(X1, X2, metric=\"euclidean\").mean()\n",
    "    xx = pairwise_distances(X1,        metric=\"euclidean\").mean()\n",
    "    yy = pairwise_distances(X2,        metric=\"euclidean\").mean()\n",
    "\n",
    "    return 2*xy - xx - yy        # scalar float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing metrics for the predicted data against the stimulated data\n",
    "X = eval_ad.X.toarray() if sparse.issparse(eval_ad.X) else eval_ad.X # Converting the data to a dense array if it is sparse\n",
    "ctrl_m = eval_ad.obs['condition']=='control' # Creating a mask for the control condition\n",
    "stim_m = eval_ad.obs['condition']=='stimulated' # Creating a mask for the stimulated condition\n",
    "pred_m = eval_ad.obs['condition']=='predicted' # Creating a mask for the predicted condition\n",
    "\n",
    "# R^2, MSE, and Pearson correlation coefficients calculation\n",
    "boot = bootstrap_metrics(X, stim_m, pred_m, eval_ad.obsm['X_pca']) # Bootstrapping metrics for the predicted data against the stimulated data\n",
    "\n",
    "# Energy distance calculation\n",
    "Xp = eval_ad.obsm[\"X_pca\"] # Getting the PCA coordinates\n",
    "stim_ids  = np.random.default_rng(SEED).choice(np.where(stim_m)[0], 2000, replace=False) # Randomly selecting 2000 stimulated cells for energy distance calculation\n",
    "Xp_stim_sub = Xp[stim_ids] # cached subset\n",
    "e_dist = energy_d(Xp[pred_m], Xp_stim_sub) # Computing the energy distance between the predicted and stimulated data\n",
    "\n",
    "# Mean-variance KDE distance calculation\n",
    "mv_kde = mean_var_kde_distance(X[stim_m], X[pred_m]) # Computing the mean-variance KDE distance between the stimulated and predicted data\n",
    "\n",
    "# Top 100 DEG Jaccard index calculation\n",
    "sc.tl.rank_genes_groups(eval_ad, groupby='condition', reference='control',\n",
    "                        groups=['stimulated','predicted'], method='wilcoxon', n_genes=eval_ad.n_vars) # Ranking genes for the stimulated and predicted conditions against the control condition\n",
    "true_top = eval_ad.uns['rank_genes_groups']['names']['stimulated'][:100] # Getting the top 100 genes for the stimulated condition\n",
    "pred_top = eval_ad.uns['rank_genes_groups']['names']['predicted'][:100] # Getting the top 100 genes for the predicted condition\n",
    "shared = set(true_top)&set(pred_top) # Finding the shared top genes between the stimulated and predicted conditions\n",
    "jaccard = len(shared)/(200-len(shared)) # Computing the Jaccard index for the shared top genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame to store the results\n",
    "results_df = pd.DataFrame([{\n",
    "    'cell_type'      : 'CD4 T cells',\n",
    "    'gene_set'       : 'CD4 default',\n",
    "    **boot,\n",
    "    'e_distance'     : e_dist,\n",
    "    'mv_kde'         : mv_kde,\n",
    "    'jaccard_top100' : jaccard\n",
    "}])\n",
    "\n",
    "results_df # Displaying the results DataFrame with the computed metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87f9f13",
   "metadata": {},
   "source": [
    "**Results Summary:**\n",
    "\n",
    "| Metric                         |                Result | Quick read-out                                                                                                                                                                                                                                                                                                                                         |\n",
    "| ------------------------------ | --------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **R²**                         |              **0.92** | ≈ 92 % of the gene-mean variance of real IFN-β cells is captured. Excellent.                                                                                                                                                                                                                                                                           |\n",
    "| **MSE**                        | **0.0066 log-counts²** | Errors are tiny; on average the prediction deviates by √0.0066 ≈ 0.08 log-counts per gene.                                                                                                                                                                                                                                                              |\n",
    "| **Pearson r**                  |              **0.96** | Gene-means rise and fall almost exactly in sync with the true stimulation profile.                                                                                                                                                                                                                                                                     |\n",
    "| **Energy distance (E-distance)**            |              **2.10** | 0 would mean the two full distributions are identical; ≈2 is already smaller than typical between-condition distances in PBMCs, so the synthetic cells sit close to the real IFN-β cloud.                                                                                                                                                          |\n",
    "| **Mean-variance KDE distance** |             **0.026** | The shape of the mean/variance landscape is nearly indistinguishable between predicted and real INF-β cells.                                                                                                                                                                                                                                                 |\n",
    "| **Top-100 DEG Jaccard**        |     **0.087** (≈ 9 %) | Only 9 of the top 100 differentially expressed genes overlap. This sounds low, but remember we **down-selected the data set to just 2,000 highly variable genes** for speed in the workshop. Many true IFN-β markers (e.g. *IFI27*, *IFITM1*) may have been pruned away. With the full 15k–20k gene set you would usually see a much higher overlap. |\n",
    "\n",
    "- scGen captures the **global transcriptional shift** of IFN-β stimulation in CD4 T cells (high R², Pearson, low MSE, small energy/KDE distances).\n",
    "- The **specific gene-ranking agreement** is modest here because of our tutorial shortcut (2,000 HVGs). In a real benchmark you would keep more genes or compute the Jaccard on the union of detected DEGs to get a fairer picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289e332",
   "metadata": {},
   "source": [
    "We save the results to a TSV file for later inspection and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0fa8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"../data/scGen_metrics_CD4_default.tsv\", sep=\"\\t\", index=False) # Saving the results DataFrame to a TSV file for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996de8da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc2cfc",
   "metadata": {},
   "source": [
    "### 5.2. Visualising scGen Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a58be8",
   "metadata": {},
   "source": [
    "#### 5.2.1. Mean correlation of gene-level responses (R²)\n",
    "\n",
    "The numerical scores above tell us the prediction is good, but a picture makes it obvious *where* the model succeeds or fails. Below we compute, **for every gene**, its average log-expression in real IFN-β CD4 T cells (`stimulated`) and scGen-predicted CD4 T cells (`predicted`), and we draw a scatter-plot of those two vectors. The closer the points hug the diagonal, the better the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa097a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1) per-gene means and R² ─────────────────────────────────────────────\n",
    "stim_means = X[stim_m].mean(axis=0)        # ground-truth\n",
    "pred_means = X[pred_m].mean(axis=0)        # scGen estimate\n",
    "r2 = r2_score(stim_means, pred_means)\n",
    "\n",
    "# wrap into a DataFrame so we can use seaborn + gene names\n",
    "df = pd.DataFrame(\n",
    "    {\"pred\": pred_means,\n",
    "     \"real\": stim_means},\n",
    "    index = eval_ad.var_names            # gene names as index\n",
    ")\n",
    "\n",
    "# ── 2) basic scatter with regression ribbon ──────────────────────────────\n",
    "sns.set_style(\"whitegrid\")               # light grid\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "ax = sns.regplot(\n",
    "    data       = df,\n",
    "    x          = \"pred\",\n",
    "    y          = \"real\",\n",
    "    scatter_kws= dict(s=10, alpha=0.6),\n",
    "    line_kws   = dict(color=\"navy\"),\n",
    "    ci         = 95                      # 95 % conf. band\n",
    ")\n",
    "\n",
    "# reference diagonal\n",
    "data_min = min(df.min())\n",
    "data_max = max(df.max())\n",
    "pad      = 0.02 * (data_max - data_min)      # 2 % of the data range\n",
    "lims     = [data_min - pad, data_max + pad]\n",
    "\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)\n",
    "ax.plot(lims, lims, \"--\", color=\"grey\", linewidth=1)\n",
    "\n",
    "# ── 3) annotate “interesting” genes (> 1 log-mean in either set) ─────────\n",
    "mask = (df[\"pred\"] > 1) | (df[\"real\"] > 1)\n",
    "for gene, row in df[mask].iterrows():\n",
    "    ax.annotate(gene,\n",
    "                (row[\"pred\"], row[\"real\"]),\n",
    "                xytext=(4,4),\n",
    "                textcoords=\"offset points\",\n",
    "                fontsize=8)\n",
    "\n",
    "# axes & title\n",
    "ax.set_xlim(lims); ax.set_ylim(lims)\n",
    "ax.set_xlabel(\"scGen predicted (log mean)\")\n",
    "ax.set_ylabel(\"Real IFN-β (log mean)\")\n",
    "ax.set_title(f\"Gene-level mean correlation - $R^2$ = {r2:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2e87c",
   "metadata": {},
   "source": [
    "* **One dot = one gene** (2,000 HVGs).\n",
    "* **Two diagonal guides:**\n",
    "  * **Grey dashed** $y = x$: the *ideal* 1 : 1 match.\n",
    "  * **Blue solid**: least-squares fit $\\text{real} = \\beta \\, \\text{pred} + \\alpha$.\n",
    "    Here **β is a bit >1**, so the blue line sits **above** the grey one at higher values → scGen **slightly under-predicts** the largest fold-changes; $\\alpha \\approx 0$ means no offset at low expression.\n",
    "* **Point patterns:**\n",
    "  * Dense cloud near (0, 0): low-expressed genes are reproduced almost perfectly.\n",
    "  * Genes with log-means ≈ 1–6 show the IFN-β response; mostly fall on the diagonal, a few outliers mark minor over- or under-estimation.\n",
    "* **Overall score – $R^2 \\approx 0.92$**: About 92 % of the variance in real gene means is explained by scGen, confirming an excellent global fit despite the slight under-prediction of the strongest responders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c7c86",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be8247e",
   "metadata": {},
   "source": [
    "#### 5.2.2. Energy distance heatmap\n",
    "\n",
    "Energy distance (E-distance) compares entire **multivariate distributions**, not just their means. If scGen is doing its job, the distribution of synthetic predicted CD4 T cells should sit much closer to the real stimulated cells than either of them sits to the control cells. A 3×3 heat-map of pair-wise E-distances makes that relationship obvious at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1) Pair-wise Energy-distance matrix  (control / stimulated / predicted)\n",
    "# ------------------------------------------------------------------\n",
    "group_repr = {\n",
    "    \"control\"     : Xp[ctrl_m],\n",
    "    \"stimulated\"  : Xp_stim_sub,      \n",
    "    \"predicted\"   : Xp[pred_m],\n",
    "}\n",
    "\n",
    "groups = [\"control\", \"stimulated\", \"predicted\"]\n",
    "E_df   = pd.DataFrame(index=groups, columns=groups, dtype=float)\n",
    "\n",
    "for i, g1 in enumerate(groups):\n",
    "    X1 = group_repr[g1]\n",
    "    for g2 in groups[i:]:\n",
    "        X2  = group_repr[g2]\n",
    "        ed  = energy_d(X1, X2)   \n",
    "        E_df.loc[g1, g2] = E_df.loc[g2, g1] = ed\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Heat-map visual\n",
    "# ------------------------------------------------------------------\n",
    "sns.set(font_scale=1.1)\n",
    "plt.figure(figsize=(6, 5))\n",
    "ax = sns.heatmap(\n",
    "    E_df.astype(float), annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "    linewidths=0.7, square=True,\n",
    "    cbar_kws=dict(label=\"E-distance\", shrink=0.9, pad=0.03)\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"\")                       # remove redundant y-label\n",
    "ax.set_title(\"Pair-wise E-distance (Energy-d)\", pad=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a535ca51",
   "metadata": {},
   "source": [
    "- The diagonal is 0 by definition (a distribution compared with itself).\n",
    "- `stimulated ↔ predicted` shows the smallest value.\n",
    "- Both `control ↔ stimulated` and `control ↔ predicted` are noticeably larger, indicating that the synthetic cells really do resemble the true IFN-β response rather than the untreated state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ffeac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c03b68",
   "metadata": {},
   "source": [
    "#### 5.2.3. Bubble plot of top differentially expressed genes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5612056c",
   "metadata": {},
   "source": [
    "We will (i) re-run a Wilcoxon test on **stimulated vs control** CD4 T cells, (ii) pull the **10 most significant DE genes**, and (iii) feed them to `sc.pl.dotplot`. The resulting “bubble” plot shows **all three conditions** (control, predicted, stimulated) side-by-side:\n",
    "\n",
    "| Visual Cue              | Biological Meaning                                                    |\n",
    "| ----------------------- | --------------------------------------------------------------------- |\n",
    "| **Colour** (blue scale) | log-expression of the gene *within that condition* (darker = higher). |\n",
    "| **Bubble size**         | fraction of cells in which the gene is detected (>0 UMIs).            |\n",
    "\n",
    "If scGen has done a good job, the **predicted** column should look almost identical to the **stimulated** one, both in hue *and* in bubble size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31edd628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1) Identify the top-N IFN-β DE genes ──────────────────────────────\n",
    "# re-run the test on the evaluation AnnData (control / stimulated / predicted)\n",
    "sc.tl.rank_genes_groups(\n",
    "    eval_ad,\n",
    "    groupby      = \"condition\",\n",
    "    reference    = \"control\",\n",
    "    groups       = [\"stimulated\"],     # DEGs *of* the real IFN-β cells\n",
    "    n_genes      = 10,                 # top-10 is plenty for a demo\n",
    "    method       = \"wilcoxon\"\n",
    ")\n",
    "\n",
    "top_genes = eval_ad.uns[\"rank_genes_groups\"][\"names\"][\"stimulated\"][:10]\n",
    "\n",
    "# ── 2) Bubble plot ────────────────────────────────────────────────────\n",
    "dp = sc.pl.DotPlot(\n",
    "    eval_ad,\n",
    "    var_names=top_genes, # top-10 DEGs from the stimulated condition\n",
    "    groupby=\"condition\", # group by condition\n",
    "    standard_scale=\"var\", # standardize across genes (var)\n",
    "    cmap=\"Blues\", \n",
    ")\n",
    "\n",
    "dp.make_figure() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7835c",
   "metadata": {},
   "source": [
    "**How to read the DEGs bubble plot:**\n",
    "\n",
    "* Large, dark-blue circles in the **stimulated** column confirm that canonical IFN-β genes (e.g. *ISG15, IFIT3*) are strongly induced and broadly expressed.\n",
    "* In the **control** column the same genes appear tiny and pale; exactly what we expect in untreated cells.\n",
    "* The key check: does the **predicted** column mirror the stimulated pattern?\n",
    "\n",
    "  * If the colours and bubble sizes line up, scGen has not only matched the **mean expression** but also recaptured the **fraction of responding cells**.\n",
    "  * Any gene where the predicted bubble is visibly lighter or smaller flags a concrete shortfall in the model’s reconstruction of the IFN-β response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5208e028",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081237b",
   "metadata": {},
   "source": [
    "## 6. Plug-&-Play: scGen for any cell-type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe0b352",
   "metadata": {},
   "source": [
    "The scGen model we just trained is specific to CD4 T cells, but the same pipeline can be applied to any other lineage in the *Kang 2018* dataset. But how does choosing a different cell type affect the results? Let’s find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7491a6f",
   "metadata": {},
   "source": [
    "**How to use the following piece of code:**\n",
    "1. Run the two following code cells one after the other.\n",
    "2. Pick one or many cell-types from the list (using Ctrl + Click).\n",
    "3. Click the **Run scGen** button to execute the code and train scGen on the selected cell type(s).\n",
    "4. Wait for the training to finish (it should as much time as one run of training scGen on CD4 T cells × the number of cell types you selected).\n",
    "5. A table and six side-by-side bar-plots will appear so you can compare the metrics (R², MSE, Pearson, E-distance, MV-KDE, Jaccard-top100) across cell types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d18cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║  Helper: train scGen for ONE cell type and return evaluation AnnData ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "def run_scgen_for_celltype(\n",
    "    adata: anndata.AnnData,\n",
    "    celltype: str,\n",
    "    n_genes: int | None = 2000,          # None = all genes\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "    epochs: int = 10,\n",
    "    batch_size: int = 32\n",
    ") -> anndata.AnnData:\n",
    "    \"\"\"\n",
    "    1.  Hold out *stimulated* cells of <celltype>.\n",
    "    2.  Train scGen on the rest.\n",
    "    3.  Predict the held-out condition.\n",
    "    4.  Concatenate control, stimulated, predicted cells.\n",
    "    5.  Return that eval_adata (with PCA in .obsm['X_pca']).\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) build training set ------------------------------------------\n",
    "    train_adata = adata[~(\n",
    "        (adata.obs[\"cell_type\"] == celltype) &\n",
    "        (adata.obs[\"condition\"] == \"stimulated\")\n",
    "    )].copy()\n",
    "\n",
    "    # optional HVG selection for speed\n",
    "    if n_genes is not None:\n",
    "        sc.pp.highly_variable_genes(\n",
    "            train_adata, n_top_genes=n_genes,\n",
    "            flavor=\"seurat_v3\", subset=True\n",
    "        )\n",
    "\n",
    "    # --- 2) scGen setup / train -----------------------------------------\n",
    "    pt.tl.Scgen.setup_anndata(\n",
    "        train_adata,\n",
    "        batch_key  = \"condition\",\n",
    "        labels_key = \"cell_type\"\n",
    "    )\n",
    "\n",
    "    model = pt.tl.Scgen(train_adata)\n",
    "    model.train(\n",
    "        max_epochs     = epochs,\n",
    "        batch_size     = batch_size,\n",
    "        early_stopping = True,\n",
    "        accelerator    = (\"cpu\"),\n",
    "        devices        = 1\n",
    "    )\n",
    "\n",
    "    # --- 3) predict held-out stimulated state ---------------------------\n",
    "    pred, _ = model.predict(\n",
    "        ctrl_key           = \"control\",\n",
    "        stim_key           = \"stimulated\",\n",
    "        celltype_to_predict= celltype\n",
    "    )\n",
    "    pred.obs[\"condition\"] = \"predicted\"\n",
    "\n",
    "    # --- 4) build evaluation AnnData ------------------------------------\n",
    "    ctrl = adata[(adata.obs[\"cell_type\"]==celltype)&(adata.obs[\"condition\"]==\"control\")]\n",
    "    stim = adata[(adata.obs[\"cell_type\"]==celltype)&(adata.obs[\"condition\"]==\"stimulated\")]\n",
    "\n",
    "    eval_ad = anndata.concat([ctrl, stim, pred])\n",
    "    eval_ad.obs_names_make_unique()\n",
    "\n",
    "    # quick PCA for metric helpers\n",
    "    sc.tl.pca(eval_ad, n_comps=30, svd_solver=\"arpack\")\n",
    "    return eval_ad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a57d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════════════════════════╗\n",
    "# ║  Plug-&-Play: choose cell types, train & evaluate automatically    ║\n",
    "# ╚════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "# ---------------------------------------\n",
    "# 0) widget for multi-selection\n",
    "# ---------------------------------------\n",
    "avail_ct = sorted(adata.obs[\"cell_type\"].unique())\n",
    "ct_select = widgets.SelectMultiple(\n",
    "    options = avail_ct,\n",
    "    description = \"cell types\",\n",
    "    rows = 8,\n",
    "    style = {\"description_width\":\"110px\"},\n",
    "    disabled = False\n",
    ")\n",
    "run_btn = widgets.Button(description=\"Run scGen\", button_style=\"success\")\n",
    "display(widgets.VBox([ct_select, run_btn]))\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1) callback: train/evaluate on click\n",
    "# ---------------------------------------\n",
    "def on_click_run(b):\n",
    "    # guard\n",
    "    if len(ct_select.value) == 0:\n",
    "        print(\"👇 Select at least one cell type first.\")\n",
    "        return\n",
    "\n",
    "    sel_cts   = list(ct_select.value)\n",
    "    print(f\"Training scGen for: {', '.join(sel_cts)}\")\n",
    "\n",
    "    results = []\n",
    "    for ct in sel_cts:\n",
    "        eval_ad = run_scgen_for_celltype(\n",
    "            adata,                      # full dataset after QC/HVG\n",
    "            celltype = ct,\n",
    "            n_genes  = 2000,\n",
    "            device   = device,\n",
    "            epochs   = 10,\n",
    "            batch_size = 32\n",
    "        )\n",
    "\n",
    "        # masks\n",
    "        X      = eval_ad.X.toarray() if sparse.issparse(eval_ad.X) else eval_ad.X\n",
    "        Xp     = eval_ad.obsm[\"X_pca\"] # 30-PC representation\n",
    "        stim_m = eval_ad.obs[\"condition\"] == \"stimulated\"\n",
    "        pred_m = eval_ad.obs[\"condition\"] == \"predicted\"\n",
    "\n",
    "        # metrics\n",
    "        boot  = bootstrap_metrics(X, stim_m, pred_m, Xp)\n",
    "        \n",
    "        rng        = np.random.default_rng(SEED)\n",
    "        stim_ids   = rng.choice(np.where(stim_m)[0], 2_000, replace=False)\n",
    "        Xp_stimsub = Xp[stim_ids]\n",
    "        e_dist     = energy_d(Xp[pred_m], Xp_stimsub)   # no internal sub-sampling\n",
    "\n",
    "        mv_kde = mean_var_kde_distance(X[stim_m], X[pred_m])\n",
    "\n",
    "        sc.tl.rank_genes_groups(\n",
    "            eval_ad,\n",
    "            groupby   = \"condition\",\n",
    "            reference = \"control\",\n",
    "            groups    = [\"stimulated\", \"predicted\"],\n",
    "            method    = \"wilcoxon\",\n",
    "            n_genes   = eval_ad.n_vars\n",
    "        )\n",
    "        true_top = eval_ad.uns[\"rank_genes_groups\"][\"names\"][\"stimulated\"][:100]\n",
    "        pred_top = eval_ad.uns[\"rank_genes_groups\"][\"names\"][\"predicted\" ][:100]\n",
    "        shared   = set(true_top) & set(pred_top)\n",
    "        jaccard  = len(shared) / (200 - len(shared))\n",
    "\n",
    "        results.append({\n",
    "            \"cell_type\" : ct,\n",
    "            \"R2\"        : boot[\"R2\"],\n",
    "            \"MSE\"       : boot[\"MSE\"],\n",
    "            \"Pearson\"   : boot[\"Pearson\"],\n",
    "            \"E_distance\": e_dist,\n",
    "            \"MV_KDE\"    : mv_kde,\n",
    "            \"Jaccard\"   : jaccard\n",
    "        })\n",
    "\n",
    "    res_df = pd.DataFrame(results).set_index(\"cell_type\")\n",
    "    display(res_df.style.format(\"{:.3f}\"))\n",
    "\n",
    "    # -------- bar-plots --------\n",
    "    metrics = [\"R2\",\"MSE\",\"Pearson\",\"E_distance\",\"MV_KDE\",\"Jaccard\"]\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(14, 6))\n",
    "    axes = axes.ravel()\n",
    "    for ax, m in zip(axes, metrics):\n",
    "        sns.barplot(x=res_df.index, y=res_df[m], ax=ax, palette=\"Blues_d\")\n",
    "        ax.set_title(m); ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    fig.suptitle(\"scGen performance by cell type\", y=1.02, fontsize=14)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "run_btn.on_click(on_click_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea77d826",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b9368f",
   "metadata": {},
   "source": [
    "## 7. Cross-Study Extrapolation with scGen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e06558",
   "metadata": {},
   "source": [
    "- In real-world scenarios, it is very likely that **perturbation modelling** will be needed to generate perturbation predictions for a control cell population coming from a completely different dataset than the one used for training.  \n",
    "  For example: *What would happen if some control cells from outside of the Kang 2018 PBMC dataset were treated with IFN-β?*\n",
    "\n",
    "- We are coining the term **\"cross-study extrapolation\"** to describe this particular scenario. A minor adjustment to the **scGen** workflow can facilitate this type of analysis.\n",
    "\n",
    "- One important consideration is the **batch effect** that naturally arises when combining two datasets (training and testing), which may act as a confounding factor.  \n",
    "  Therefore, this tutorial will explore cross-study extrapolation in both:\n",
    "  - *integrated* training/testing datasets using `scVI` / `scANVI`\n",
    "  - *unintegrated* training/testing datasets\n",
    "\n",
    "- The testing dataset will again be **PBMCs ± IFN-β** from the [CINEMA-OT publication](https://www.nature.com/articles/s41592-023-02040-5).  \n",
    "  We provide both the unintegrated versions of the two datasets and their integrated versions using `scVI` / `scANVI`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d312a572",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7913618",
   "metadata": {},
   "source": [
    "### 7.1. Ιntegrated Cross-Study Extrapolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3250b11d",
   "metadata": {},
   "source": [
    "#### 7.1.1. Integrated Data Exploration and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686987cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_scvi = sc.read(\"../data/Kang_Dong_integrated_filtered_final.h5ad\") # loading the integrated and filtered Kang - Dong dataset\n",
    "\n",
    "adata_scvi.obs # viewing the cell metadata of the integrated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_scvi.obs[\"dataset\"] = adata_scvi.obs['batch'].apply(lambda x: \"Kang\" if \"Kang\" in x else \"Dong\") # Adding a new column 'dataset' to the metadata based on the 'batch' column\n",
    "\n",
    "adata_scvi # viewing the integrated dataset after adding the 'dataset' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_scvi.obs # viewing the cell metadata of the integrated dataset after adding the 'dataset' column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5debf92f",
   "metadata": {},
   "source": [
    "We first need first to switch to the scvi normalized matrix and apply log1p transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca9b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_scvi.layers[\"scvi_normalized_log1p\"] = np.log1p(adata_scvi.layers[\"scvi_normalized\"]) # Adding a new layer 'scvi_normalized_log1p' to the integrated dataset, which is the log1p transformation of the 'scvi_normalized' layer\n",
    "adata_scvi.X = adata_scvi.layers[\"scvi_normalized_log1p\"] # Setting the 'X' attribute of the integrated dataset to the 'scvi_normalized_log1p' layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b02d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_scvi, color=\"batch\") # Plotting UMAP of the integrated dataset colored by 'batch'\n",
    "sc.pl.umap(adata_scvi, color=\"cell_type\") # Plotting UMAP of the integrated dataset colored by 'cell_type'\n",
    "sc.pl.umap(adata_scvi, color=\"dataset\") # Plotting UMAP of the integrated dataset colored by 'dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c2b37",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747ba5bd",
   "metadata": {},
   "source": [
    "#### 7.1.2. Training and Evaluating scGen on the Integrated Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323daee5",
   "metadata": {},
   "source": [
    "We start by defining which is the training dataset (Kang) and which is the testing dataset (Dong)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b78a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = adata_scvi[(adata_scvi.obs[\"dataset\"] == \"Kang\") & (adata_scvi.obs[\"perturbation\"].isin([\"IFNb\", \"ctrl\"]))].copy() # Creating a training set by filtering the integrated dataset for 'Kang' dataset and 'IFNb' or 'ctrl' perturbations\n",
    "query = adata_scvi[adata_scvi.obs[\"dataset\"] == \"Dong\"].copy() # Creating a query set by filtering the integrated dataset for 'Dong' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.tl.Scgen.setup_anndata(train, batch_key=\"perturbation\", labels_key=\"cell_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff003ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_CTRL = query[query.obs[\"perturbation\"].isin([\"ctrl\"])].copy()\n",
    "query_STIM = query[query.obs[\"perturbation\"].isin([\"IFNb\"])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9154113",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = pt.tl.Scgen(train)\n",
    "\n",
    "# this step is necessary if your target dataset has more cell types than your training dataset (include link here)\n",
    "new_model._register_manager_for_instance(new_model.adata_manager.transfer_fields(adata_target=query_CTRL, extend_categories=True))\n",
    "\n",
    "\n",
    "new_model.train(\n",
    "    max_epochs=10,\n",
    "    batch_size=32,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=25,\n",
    "    accelerator=('cpu'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aca460",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, delta = new_model.predict(adata_to_predict=query_CTRL, ctrl_key=\"ctrl\", stim_key=\"IFNb\")\n",
    "pred.obs[\"perturbation\"] = \"pred\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_adata = query_CTRL.concatenate(query_STIM,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_adata.obs['perturbation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1911f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.pca(eval_adata)\n",
    "sc.pl.pca(eval_adata, color = [\"perturbation\",\"cell_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f96d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_r2_mean_variance_by_celltype(\n",
    "    adata,\n",
    "    condition_col: str = \"condition\",\n",
    "    celltype_col: str = \"cell_type\",\n",
    "    cond_a: str = \"eoPE\",\n",
    "    cond_b: str = \"pred\",\n",
    "    title: str = None,\n",
    "    consistent_order: bool = True,\n",
    "    figsize: tuple = None\n",
    "):\n",
    "    r2_mean_dict = {}\n",
    "    r2_var_dict = {}\n",
    "\n",
    "    for cell_type in adata.obs[celltype_col].unique():\n",
    "        adata_sub = adata[adata.obs[celltype_col] == cell_type]\n",
    "\n",
    "        if not set([cond_a, cond_b]).issubset(adata_sub.obs[condition_col].unique()):\n",
    "            continue\n",
    "\n",
    "        a_cells = adata_sub[adata_sub.obs[condition_col] == cond_a]\n",
    "        b_cells = adata_sub[adata_sub.obs[condition_col] == cond_b]\n",
    "\n",
    "        X_a = a_cells.raw.X if a_cells.raw is not None else a_cells.X\n",
    "        X_b = b_cells.raw.X if b_cells.raw is not None else b_cells.X\n",
    "\n",
    "        X_a = X_a.toarray() if hasattr(X_a, \"toarray\") else X_a\n",
    "        X_b = X_b.toarray() if hasattr(X_b, \"toarray\") else X_b\n",
    "\n",
    "        mean_a = np.mean(X_a, axis=0).flatten()\n",
    "        mean_b = np.mean(X_b, axis=0).flatten()\n",
    "        var_a = np.var(X_a, axis=0).flatten()\n",
    "        var_b = np.var(X_b, axis=0).flatten()\n",
    "\n",
    "        # Skip if variance is constant (to avoid linregress error)\n",
    "        if np.max(var_b) == np.min(var_b) or np.max(var_a) == np.min(var_a):\n",
    "            continue\n",
    "\n",
    "        _, _, r_mean, _, _ = linregress(mean_b, mean_a)\n",
    "        _, _, r_var, _, _ = linregress(var_b, var_a)\n",
    "\n",
    "        r2_mean_dict[cell_type] = r_mean ** 2\n",
    "        r2_var_dict[cell_type] = r_var ** 2\n",
    "\n",
    "    # Create DataFrame and align\n",
    "    df_mean = pd.DataFrame.from_dict(r2_mean_dict, orient='index', columns=['R2_mean'])\n",
    "    df_var = pd.DataFrame.from_dict(r2_var_dict, orient='index', columns=['R2_var'])\n",
    "    df_all = df_mean.join(df_var)\n",
    "\n",
    "    # Get cell type order\n",
    "    if consistent_order and pd.api.types.is_categorical_dtype(adata.obs[celltype_col]):\n",
    "        celltype_order = [ct for ct in adata.obs[celltype_col].cat.categories if ct in df_all.index]\n",
    "    else:\n",
    "        celltype_order = sorted(df_all.index.tolist())\n",
    "\n",
    "    df_all = df_all.loc[celltype_order]\n",
    "\n",
    "    # Set default figsize based on number of cell types\n",
    "    if figsize is None:\n",
    "        figsize = (12, len(df_all) * 0.4)\n",
    "\n",
    "    # Get colors\n",
    "    color_map = dict(zip(\n",
    "        adata.obs[celltype_col].cat.categories,\n",
    "        adata.uns[f'{celltype_col}_colors']\n",
    "    ))\n",
    "    bar_colors = [color_map[ct] for ct in df_all.index]\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize, sharey=True)\n",
    "\n",
    "    axes[0].barh(df_all.index, df_all['R2_mean'], color=bar_colors)\n",
    "    axes[0].set_xlabel(\"R²\")\n",
    "    axes[0].set_title(\"R² of Mean Expression\")\n",
    "    axes[0].invert_yaxis()\n",
    "\n",
    "    axes[1].barh(df_all.index, df_all['R2_var'], color=bar_colors)\n",
    "    axes[1].set_xlabel(\"R²\")\n",
    "    axes[1].set_title(\"R² of Variance\")\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.grid(True, axis='x', linestyle='--', alpha=0.5)\n",
    "        ax.tick_params(labelsize=10)\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ab530",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_r2_mean_variance_by_celltype(\n",
    "    adata=eval_adata,\n",
    "    condition_col=\"perturbation\",\n",
    "    celltype_col=\"cell_type\",\n",
    "    cond_a=\"IFNb\",\n",
    "    cond_b=\"pred\",\n",
    "    title=\"Perturbation R²: Ground Truth vs Pred\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4253c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a822fb6",
   "metadata": {},
   "source": [
    "### 7.2. Unintegrated Data Cross-Study Extrapolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd1cca2",
   "metadata": {},
   "source": [
    "#### 7.2.1. Unintegrated Data Exploration and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3972909",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_scvi = sc.read(\"../data/unintegrated_Kang_Dong.h5ad\")  # loading the unintegrated Kang - Dong dataset\n",
    "\n",
    "adata_scvi.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_scvi.obs[\"dataset\"] = adata_scvi.obs['batch'].apply(lambda x: \"Kang\" if \"Kang\" in x else \"Dong\")\n",
    "\n",
    "adata_scvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_scvi.obs # viewing the cell metadata of the integrated dataset after adding the 'dataset' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_scvi.layers[\"scvi_normalized_log1p\"] = np.log1p(adata_scvi.layers[\"scvi_normalized\"]) # Adding a new layer 'scvi_normalized_log1p' to the integrated dataset, which is the log1p transformation of the 'scvi_normalized' layer\n",
    "adata_scvi.X = adata_scvi.layers[\"scvi_normalized_log1p\"] # Setting the 'X' attribute of the integrated dataset to the 'scvi_normalized_log1p' layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9283e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_scvi, color=\"batch\")\n",
    "sc.pl.umap(adata_scvi, color=\"cell_type\")\n",
    "sc.pl.umap(adata_scvi, color=\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19199e4e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98475fa0",
   "metadata": {},
   "source": [
    "#### 7.2.2. Training and Evaluating scGen on the Unintegrated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9941bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = adata_scvi[(adata_scvi.obs[\"dataset\"] == \"Kang\") & (adata_scvi.obs[\"perturbation\"].isin([\"IFNb\", \"ctrl\"]))].copy()\n",
    "query = adata_scvi[adata_scvi.obs[\"dataset\"] == \"Dong\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55118e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.tl.Scgen.setup_anndata(train, batch_key=\"perturbation\", labels_key=\"cell_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d35272",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_CTRL = query[query.obs[\"perturbation\"].isin([\"ctrl\"])].copy()\n",
    "query_STIM = query[query.obs[\"perturbation\"].isin([\"IFNb\"])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac24bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = pt.tl.Scgen(train)\n",
    "\n",
    "# this step is necessary if your target dataset has more cell types than your training dataset (include link here)\n",
    "new_model._register_manager_for_instance(new_model.adata_manager.transfer_fields(adata_target=query_CTRL, extend_categories=True))\n",
    "\n",
    "\n",
    "new_model.train(\n",
    "    max_epochs=100,\n",
    "    batch_size=32,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=25,\n",
    "    accelerator=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, delta = new_model.predict(adata_to_predict=query_CTRL, ctrl_key=\"ctrl\", stim_key=\"IFNb\")\n",
    "pred.obs[\"perturbation\"] = \"pred\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84746af",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_adata = query_CTRL.concatenate(query_STIM,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ad286",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_adata.obs['perturbation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c62e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.pca(eval_adata)\n",
    "sc.pl.pca(eval_adata, color = [\"perturbation\",\"cell_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c71e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_r2_mean_variance_by_celltype(\n",
    "    adata=eval_adata,\n",
    "    condition_col=\"perturbation\",\n",
    "    celltype_col=\"cell_type\",\n",
    "    cond_a=\"IFNb\",\n",
    "    cond_b=\"pred\",\n",
    "    title=\"Perturbation R²: Ground Truth vs Pred\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea52f59",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e6eec9",
   "metadata": {},
   "source": [
    "## 8. Key take-aways\n",
    "\n",
    "* We learned how to **load, inspect and manipulate AnnData** objects: filtering low-quality genes, total-count normalising, log-transforming and selecting HVGs. We practised quick exploratory visuals (bar plots, PCA, built-in UMAP) to recognise cell-type structure and verify that metadata (`condition`, `cell_type`) are harmonised for modelling.\n",
    "* With the prepared *Kang 2018* PBMC data we *held out* IFN-β stimulated CD4 T cells, set up scGen and trained **10 epochs**. This showed how little boiler-plate is needed once the AnnData is curated, and where hyperparameters could later be tuned.\n",
    "* The model predicted the unseen IFN-β response with **R² ≈ 0.92, Pearson ≈ 0.96** and an energy-distance far smaller than control-to-stim. Even after the tutorial’s aggressive 2k-HVG pruning it recovered \\~9 % of top-100 DEGs, a strong signal-to-noise ratio given the reduced gene set.\n",
    "* The **mean-correlation scatter** made gene-by-gene accuracy tangible, while the **energy-distance heat-map** visualized the distances between the three cell groups (control, stimulated, predicted) in a way that highlights the model’s success at capturing the IFN-β response.\n",
    "* Through the **plug-&-play widget**, we were able to retrained scGen on any chosen cell type, automatically recalculating all metrics and bar-plot comparisons, and we realised that the choice of cell type modulates performance.\n",
    "* By the end of the session everyone could:\n",
    "  1. curate and explore single-cell data in AnnData,\n",
    "  2. train a landmark perturbation model,\n",
    "  3. generate, visualise and interpret in-silico predictions, and\n",
    "  4. benchmark results with statistically and biologically meaningful metrics.\n",
    "\n",
    "\n",
    "**Other things you could try:**\n",
    "| Idea                                                                | Why it’s interesting                                                                      |\n",
    "| ------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |\n",
    "| **Increase gene coverage** (e.g. 5 k or all genes)                  | Improves DEG overlap; see how metric rankings change.                                     |\n",
    "| **Hyper-parameter sweep** with Optuna                               | Quantify how latent-dim or early-stopping affect different cell types.                    |\n",
    "| **Add other perturbations/datasets** (LPS, TNF-α, CRISPR knock-outs)         | Test scGen’s versatility and limits.                                             |\n",
    "\n",
    "* **Bottom line:** Even with a beginner-friendly pipeline and modest resources, **scGen captures >90 % of the IFN-β transcriptional programme** in unseen CD4 T cells and does so consistently across other immune lineages. The notebook you just ran is a **reproducible blueprint**; adapt the preprocessing, tweak the hyper-parameters and you are ready to explore perturbation responses in your own single-cell experiments. Happy modelling!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
